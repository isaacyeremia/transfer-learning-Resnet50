{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Import Library & Download Dataset**"
      ],
      "metadata": {
        "id": "GYuNmlTokdKL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "yhNk3pJekcIt",
        "outputId": "938edf0d-adc8-43e0-8238-445e95eaf21e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset belum ditemukan.\n",
            "Silakan Upload file kaggle.json Anda:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c9ed1ca6-4ba7-4fbc-8056-f96f4d0ff494\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c9ed1ca6-4ba7-4fbc-8056-f96f4d0ff494\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "\n",
            "Mendownload dataset CUB-200-2011...\n",
            "Dataset URL: https://www.kaggle.com/datasets/wenewone/cub2002011\n",
            "License(s): CC0-1.0\n",
            "Downloading cub2002011.zip to /content\n",
            " 97% 1.45G/1.49G [00:17<00:00, 43.9MB/s]\n",
            "100% 1.49G/1.49G [00:17<00:00, 90.9MB/s]\n",
            "Mengekstrak dataset...\n",
            "Selesai download & ekstrak.\n",
            "✅ PATH DATASET DITEMUKAN DI: cub_dataset/CUB_200_2011/images\n",
            "✅ Menggunakan Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from google.colab import files\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Setup Kaggle & Download\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\"\n",
        "\n",
        "# Cek apakah folder dataset sudah ada\n",
        "if not os.path.exists('cub_dataset'):\n",
        "    print(\"Dataset belum ditemukan.\")\n",
        "\n",
        "    # Upload kaggle.json jika belum ada\n",
        "    if not os.path.exists('kaggle.json'):\n",
        "        print(\"Silakan Upload file kaggle.json Anda:\")\n",
        "        files.upload()\n",
        "        !chmod 600 /content/kaggle.json\n",
        "\n",
        "    print(\"\\nMendownload dataset CUB-200-2011...\")\n",
        "    !kaggle datasets download -d wenewone/cub2002011 --force\n",
        "    print(\"Mengekstrak dataset...\")\n",
        "    !unzip -q cub2002011.zip -d cub_dataset\n",
        "    print(\"Selesai download & ekstrak.\")\n",
        "\n",
        "# 2. FIX PATH OTOMATIS (Agar tidak FileNotFoundError)\n",
        "base_dir = 'cub_dataset'\n",
        "possible_path_1 = os.path.join(base_dir, 'images')\n",
        "possible_path_2 = os.path.join(base_dir, 'CUB_200_2011', 'images')\n",
        "\n",
        "if os.path.exists(possible_path_1):\n",
        "    data_dir = possible_path_1\n",
        "elif os.path.exists(possible_path_2):\n",
        "    data_dir = possible_path_2\n",
        "else:\n",
        "    # Fallback terakhir jika struktur zip berubah\n",
        "    data_dir = 'cub_dataset/images'\n",
        "\n",
        "print(f\"✅ PATH DATASET DITEMUKAN DI: {data_dir}\")\n",
        "\n",
        "# Cek Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"✅ Menggunakan Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Konfigurasi Hyperparameter & Augmentasi**"
      ],
      "metadata": {
        "id": "gz3N6CvWmNlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- KONFIGURASI SPEED & ACCURACY ---\n",
        "BATCH_SIZE = 32     # Aman & Cepat\n",
        "NUM_CLASSES = 200\n",
        "IMG_SIZE = 320      # Resolusi Optimal (Detail cukup, ringan di memori)\n",
        "\n",
        "# --- AUGMENTASI DATA ---\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        # Resize sedikit lebih besar dari target crop agar tidak ada border kosong\n",
        "        transforms.Resize((350, 350)),\n",
        "        # Random Crop ke 320px (memaksa model melihat bagian burung yang berbeda-beda)\n",
        "        transforms.RandomCrop(IMG_SIZE),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        # ColorJitter penting untuk dataset CUB agar model fokus ke bentuk, bukan warna background\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "print(\"✅ Konfigurasi Augmentasi Selesai\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuTvRLdhmWew",
        "outputId": "b7c1a44a-1793-4c5e-9bca-7eec95d032e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Konfigurasi Augmentasi Selesai\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Split Dataset & Membuat DataLoader**"
      ],
      "metadata": {
        "id": "_ULGjCECkwuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset Awal (Tanpa Transform Dulu)\n",
        "full_dataset = datasets.ImageFolder(data_dir)\n",
        "\n",
        "# Split 80% Train, 20% Val\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_data, val_data = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Wrapper Class (Agar Transform Train & Val tidak tertukar)\n",
        "class TransformedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.subset[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# Terapkan Transform\n",
        "train_dataset = TransformedDataset(train_data, transform=data_transforms['train'])\n",
        "val_dataset = TransformedDataset(val_data, transform=data_transforms['val'])\n",
        "\n",
        "# Buat DataLoader (pin_memory=True mempercepat transfer data ke GPU)\n",
        "dataloaders = {\n",
        "    'train': DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True),\n",
        "    'val': DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "}\n",
        "\n",
        "dataset_sizes = {'train': train_size, 'val': val_size}\n",
        "\n",
        "print(f\"✅ Data Siap!\")\n",
        "print(f\"   - Training: {train_size} gambar\")\n",
        "print(f\"   - Validasi: {val_size} gambar\")\n",
        "print(f\"   - Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"   - Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "\n",
        "# Test ambil 1 batch untuk memastikan tidak ada error\n",
        "images, labels = next(iter(dataloaders['train']))\n",
        "print(f\"   - Ukuran Tensor Batch: {images.shape}\")\n",
        "# Output harusnya: torch.Size([32, 3, 320, 320])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HgxtFptnk1BO",
        "outputId": "d1a943d6-d531-4912-87bd-8a0f2cfac52d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Data Siap!\n",
            "   - Training: 9436 gambar\n",
            "   - Validasi: 2360 gambar\n",
            "   - Batch Size: 32\n",
            "   - Image Size: 320x320\n",
            "   - Ukuran Tensor Batch: torch.Size([32, 3, 320, 320])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Loop dengan AMP (Automatic Mixed Precision)**"
      ],
      "metadata": {
        "id": "pwGJELXEnGCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import time # Import the time module\n",
        "import copy # Import the copy module, as it's also used in this function\n",
        "from tqdm import tqdm # Import tqdm for progress bar\n",
        "\n",
        "def train_fast_amp(model, criterion, optimizer, scheduler=None, num_epochs=10, name=\"Model\"):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    history = {'val_acc': []}\n",
        "\n",
        "    # Inisialisasi Scaler untuk Mixed Precision\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    print(f\"\\n{'='*10} TRAINING: {name} (AMP + Res 320px) {'='*10}\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_corrects = 0\n",
        "\n",
        "        # TQDM Progress Bar\n",
        "        loop_train = tqdm(dataloaders['train'], leave=True, desc=f\"Ep {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for inputs, labels in loop_train:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # --- MIXED PRECISION BLOCK (MAGIC HAPPENS HERE) ---\n",
        "            with autocast():\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backprop menggunakan Scaler\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            # --------------------------------------------------\n",
        "\n",
        "            train_corrects += torch.sum(preds == labels.data)\n",
        "            loop_train.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Step Scheduler di akhir epoch (jika bukan OneCycleLR)\n",
        "        # Jika OneCycleLR, step harus di dalam batch loop.\n",
        "        # Disini kita pakai step per epoch untuk simplifikasi fine tuning.\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        epoch_acc_train = train_corrects.double() / dataset_sizes['train']\n",
        "\n",
        "        # Validasi\n",
        "        model.eval()\n",
        "        val_corrects = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in dataloaders['val']:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc_val = val_corrects.double() / dataset_sizes['val']\n",
        "        history['val_acc'].append(epoch_acc_val.item())\n",
        "\n",
        "        print(f\"Result: train_acc: {epoch_acc_train:.4f} - val_acc: {epoch_acc_val:.4f}\")\n",
        "\n",
        "        if epoch_acc_val > best_acc:\n",
        "            best_acc = epoch_acc_val\n",
        "            best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f'Selesai dalam {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "OcBvGBbunKzI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Eksekusi 3 Skenario**"
      ],
      "metadata": {
        "id": "nvZdE5ZinSQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TRAINING FROM SCRATCH**"
      ],
      "metadata": {
        "id": "o10WNJkanbNm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10 # Define the number of epochs\n",
        "\n",
        "# Pastikan fungsi train_fast_amp sudah dijalankan sebelumnya\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# --- SKENARIO 1: SCRATCH (Training dari Nol) ---\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\">>> MULAI SKENARIO 1: SCRATCH <<<\")\n",
        "print(\"=\"*40)\n",
        "model_scratch = models.resnet50(weights=None) # Tanpa bobot\n",
        "model_scratch.fc = nn.Linear(model_scratch.fc.in_features, NUM_CLASSES)\n",
        "model_scratch = model_scratch.to(device)\n",
        "\n",
        "# AdamW sangat bagus untuk start dari nol\n",
        "optimizer_s = optim.AdamW(model_scratch.parameters(), lr=0.001)\n",
        "\n",
        "model_scratch, hist_scratch = train_fast_amp(\n",
        "    model_scratch, criterion, optimizer_s, num_epochs=NUM_EPOCHS, name=\"Scratch\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gY5_QIA0nWEr",
        "outputId": "188ad2fb-fefe-41bc-82f7-7bf978b3279a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            ">>> MULAI SKENARIO 1: SCRATCH <<<\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-239132057.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== TRAINING: Scratch (AMP + Res 320px) ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEp 1/10:   0%|          | 0/295 [00:00<?, ?it/s]/tmp/ipython-input-239132057.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Ep 1/10: 100%|██████████| 295/295 [01:59<00:00,  2.47it/s, loss=4.88]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0099 - val_acc: 0.0127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 2/10: 100%|██████████| 295/295 [01:47<00:00,  2.75it/s, loss=4.68]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0178 - val_acc: 0.0144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 3/10: 100%|██████████| 295/295 [01:48<00:00,  2.73it/s, loss=4.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0272 - val_acc: 0.0233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 4/10: 100%|██████████| 295/295 [01:46<00:00,  2.77it/s, loss=4.51]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0415 - val_acc: 0.0263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 5/10: 100%|██████████| 295/295 [01:49<00:00,  2.71it/s, loss=4.08]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0504 - val_acc: 0.0644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 6/10: 100%|██████████| 295/295 [01:47<00:00,  2.73it/s, loss=4.14]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0645 - val_acc: 0.0691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 7/10: 100%|██████████| 295/295 [01:47<00:00,  2.74it/s, loss=4.04]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.0821 - val_acc: 0.0708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 8/10: 100%|██████████| 295/295 [01:48<00:00,  2.73it/s, loss=3.99]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.1017 - val_acc: 0.1153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 9/10: 100%|██████████| 295/295 [01:48<00:00,  2.72it/s, loss=4.77]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.1325 - val_acc: 0.1131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 10/10: 100%|██████████| 295/295 [01:48<00:00,  2.71it/s, loss=3.62]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.1424 - val_acc: 0.1428\n",
            "Selesai dalam 20m 53s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TRANSFER LEARNING (FEATURE EXTRACTION)**"
      ],
      "metadata": {
        "id": "ydA7sjzMniCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\">>> MULAI SKENARIO 2: FEATURE EXTRACTION <<<\")\n",
        "print(\"=\"*40)\n",
        "model_fe = models.resnet50(weights='IMAGENET1K_V1') # Pakai bobot ImageNet\n",
        "\n",
        "# Freeze (Bekukan) semua layer kecuali yang terakhir\n",
        "for param in model_fe.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "model_fe.fc = nn.Linear(model_fe.fc.in_features, NUM_CLASSES)\n",
        "model_fe = model_fe.to(device)\n",
        "\n",
        "# Optimizer hanya mengupdate layer FC (Classifier)\n",
        "optimizer_fe = optim.AdamW(model_fe.fc.parameters(), lr=0.001)\n",
        "\n",
        "model_fe, hist_fe = train_fast_amp(\n",
        "    model_fe, criterion, optimizer_fe, num_epochs=NUM_EPOCHS, name=\"Feature Extraction\"\n",
        ")"
      ],
      "metadata": {
        "id": "IAUOcUu-nlpg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "199c8e0f-2a84-4973-f2c0-c7b32fdc1ed8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            ">>> MULAI SKENARIO 2: FEATURE EXTRACTION <<<\n",
            "========================================\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 195MB/s]\n",
            "/tmp/ipython-input-239132057.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== TRAINING: Feature Extraction (AMP + Res 320px) ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 1/10:   0%|          | 0/295 [00:00<?, ?it/s]/tmp/ipython-input-239132057.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Ep 1/10: 100%|██████████| 295/295 [01:33<00:00,  3.15it/s, loss=3.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.1256 - val_acc: 0.2648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 2/10: 100%|██████████| 295/295 [01:33<00:00,  3.16it/s, loss=2.74]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.3812 - val_acc: 0.3699\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 3/10: 100%|██████████| 295/295 [01:33<00:00,  3.15it/s, loss=1.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.4979 - val_acc: 0.4581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 4/10: 100%|██████████| 295/295 [01:33<00:00,  3.16it/s, loss=1.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.5656 - val_acc: 0.4581\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 5/10: 100%|██████████| 295/295 [01:34<00:00,  3.11it/s, loss=2.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.6053 - val_acc: 0.5055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 6/10: 100%|██████████| 295/295 [01:33<00:00,  3.15it/s, loss=1.09]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.6357 - val_acc: 0.5263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 7/10: 100%|██████████| 295/295 [01:34<00:00,  3.11it/s, loss=1.15]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.6572 - val_acc: 0.5318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 8/10: 100%|██████████| 295/295 [01:33<00:00,  3.14it/s, loss=1.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.6713 - val_acc: 0.5441\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 9/10: 100%|██████████| 295/295 [01:33<00:00,  3.15it/s, loss=1.7]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.6878 - val_acc: 0.5229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 10/10: 100%|██████████| 295/295 [01:34<00:00,  3.11it/s, loss=1.37]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.7041 - val_acc: 0.5297\n",
            "Selesai dalam 18m 20s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **FINE TUNING**"
      ],
      "metadata": {
        "id": "8OmbdFDVnoMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\">>> MULAI SKENARIO 3: FINE TUNING <<<\")\n",
        "print(\"=\"*40)\n",
        "model_ft = models.resnet50(weights='IMAGENET1K_V1')\n",
        "model_ft.fc = nn.Linear(model_ft.fc.in_features, NUM_CLASSES)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Settingan \"Pro\": Learning Rate berbeda tiap layer\n",
        "# Layer awal (fitur dasar) berubah pelan, Layer akhir (keputusan) berubah cepat\n",
        "optimizer_ft = optim.AdamW([\n",
        "    {'params': model_ft.conv1.parameters(), 'lr': 1e-5},\n",
        "    {'params': model_ft.layer1.parameters(), 'lr': 1e-5},\n",
        "    {'params': model_ft.layer2.parameters(), 'lr': 5e-5},\n",
        "    {'params': model_ft.layer3.parameters(), 'lr': 1e-4},\n",
        "    {'params': model_ft.layer4.parameters(), 'lr': 2e-4},\n",
        "    {'params': model_ft.fc.parameters(), 'lr': 1e-3}\n",
        "], weight_decay=0.01)\n",
        "\n",
        "# Scheduler: Menurunkan LR setiap 3 epoch agar model makin teliti\n",
        "scheduler_ft = optim.lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.2)\n",
        "\n",
        "model_ft, hist_ft = train_fast_amp(\n",
        "    model_ft, criterion, optimizer_ft, scheduler=scheduler_ft, num_epochs=NUM_EPOCHS, name=\"Fine Tuning\"\n",
        ")"
      ],
      "metadata": {
        "id": "_EQ7QVZJnrjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d56847-f359-4a16-8d0e-24173499eb7a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            ">>> MULAI SKENARIO 3: FINE TUNING <<<\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-239132057.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== TRAINING: Fine Tuning (AMP + Res 320px) ==========\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEp 1/10:   0%|          | 0/295 [00:00<?, ?it/s]/tmp/ipython-input-239132057.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "Ep 1/10: 100%|██████████| 295/295 [01:48<00:00,  2.72it/s, loss=1.48]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.3488 - val_acc: 0.5195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 2/10: 100%|██████████| 295/295 [01:47<00:00,  2.73it/s, loss=1.06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.6595 - val_acc: 0.6716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 3/10: 100%|██████████| 295/295 [01:48<00:00,  2.73it/s, loss=0.643]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.7606 - val_acc: 0.7000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 4/10: 100%|██████████| 295/295 [01:48<00:00,  2.72it/s, loss=0.412]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.8880 - val_acc: 0.8114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 5/10: 100%|██████████| 295/295 [01:47<00:00,  2.73it/s, loss=0.256]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.9170 - val_acc: 0.8182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 6/10: 100%|██████████| 295/295 [01:48<00:00,  2.73it/s, loss=0.191]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.9333 - val_acc: 0.8191\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 7/10: 100%|██████████| 295/295 [01:49<00:00,  2.69it/s, loss=0.186]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.9621 - val_acc: 0.8322\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 8/10: 100%|██████████| 295/295 [01:49<00:00,  2.69it/s, loss=0.303]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.9625 - val_acc: 0.8318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 9/10: 100%|██████████| 295/295 [01:47<00:00,  2.73it/s, loss=0.197]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.9706 - val_acc: 0.8318\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ep 10/10: 100%|██████████| 295/295 [01:48<00:00,  2.73it/s, loss=0.283]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: train_acc: 0.9721 - val_acc: 0.8314\n",
            "Selesai dalam 20m 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Visualisasi Perbandingan**"
      ],
      "metadata": {
        "id": "nCS8HH1TnuMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_scratch_max = max(hist_scratch['val_acc']) * 100\n",
        "acc_fe_max = max(hist_fe['val_acc']) * 100\n",
        "acc_ft_max = max(hist_ft['val_acc']) * 100\n",
        "\n",
        "print(\"\\n=== Ringkasan Akurasi Tertinggi (Optimized Speed) ===\")\n",
        "print(f\"Scratch: {acc_scratch_max:.2f}%\")\n",
        "print(f\"Feature Extraction: {acc_fe_max:.2f}%\")\n",
        "print(f\"Fine Tuning: {acc_ft_max:.2f}%\")"
      ],
      "metadata": {
        "id": "i9s2B-jHnwTX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06ea4f74-19cb-43be-fc31-4319a2b0200a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Ringkasan Akurasi Tertinggi (Optimized Speed) ===\n",
            "Scratch: 14.28%\n",
            "Feature Extraction: 54.41%\n",
            "Fine Tuning: 83.22%\n"
          ]
        }
      ]
    }
  ]
}